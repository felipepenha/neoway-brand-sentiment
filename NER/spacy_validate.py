# -*- coding: utf-8 -*-
"""spacy_validate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KTr0oUxy27VOldpmjxfs-zf5nGwIwmaf
"""

import ast 
from __future__ import unicode_literals, print_function

import plac
import random
from pathlib import Path
import spacy
from spacy.util import minibatch, compounding
import time
import random
from sklearn.model_selection import train_test_split
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

def create_train_data(df):
  train_data = []
  newnlp = spacy.load("en_core_web_sm")

  for i in range(len(df)):
    doc = newnlp(df['text'].iloc[i])
    entity_list = df['entities_clean'].iloc[i]
    for ent in doc.ents:
      entity_list.append((ent.start_char, ent.end_char, ent.label_))
    entity_dict = {"entities": entity_list}
    train_data.append((df['text'].iloc[i], entity_dict))
  return train_data

def create_test_data(df):
  test_data = []
  newnlp = spacy.load("en_core_web_sm")

  for i in range(len(df)):
    doc = newnlp(df['text'].iloc[i])
    entity_list = df['entities_clean'].iloc[i]
    for ent in doc.ents:
      entity_list.append((ent.start_char, ent.end_char, ent.label_))
    entity_dict = {"entities": entity_list}
    test_data.append((df['text'].iloc[i], entity_dict))
  return test_data

def create_masked_train_data(df, masked_entities):
  train_data = []
  newnlp = spacy.load("en_core_web_sm")
  
  for i in range(len(df)):
    doc = newnlp(df['text'].iloc[i])
    entity_list = df['entities_clean'].iloc[i]
    for ent in doc.ents:
      if ent.text not in masked_entities:
        entity_list.append((ent.start_char, ent.end_char, ent.label_))
    entity_dict = {"entities": entity_list}
    train_data.append((df['text'].iloc[i], entity_dict))
  return train_data

def masked_train_test(train, test):
  brand_list = []
  for (index,entity_loc) in enumerate(train['entities_clean']):
    text = train['text'].iloc[index]
    for pair in entity_loc:
      brand_list.append(text[pair[0]:pair[1]])
      
  import numpy as np
  unique_brands = np.unique(brand_list)

  newbrand_list = []
  for (index, entity_loc) in enumerate(test['entities_clean']):
    text = test['text'].iloc[index]
    for pair in entity_loc:
      newbrand_list.append(text[pair[0]:pair[1]])
      
  import numpy as np
  newunique_brands = np.unique(newbrand_list)

  in_common = list(set(unique_brands) & set(newunique_brands))
  print("Total in common:",len(in_common))

  masked_entities, unmasked_entities = train_test_split(in_common, test_size = .5)
  print("Total masked:", len(masked_entities))

  # new entity label
  TRAIN_DATA = create_masked_train_data(train_df, masked_entities)
  TEST_DATA = create_test_data(test_df)
  return TRAIN_DATA, TEST_DATA, masked_entities, unique_brands, newunique_brands

def evaluate_novelty(trained_model, masked_train_data, masked_test_data, masked_entities, unmasked_train_data, unmasked_test_data):
  nomask_true = {}
  nomask = {}

  for review in unmasked_test_data:
    test_ents_true = [review[0][start:end] for (start, end, label) in review[1]['entities']]
    doc = trained_model(review[0])
    test_ents = [ent.text for ent in doc.ents]

    for entity in masked_entities:
      if entity in test_ents_true: 
        if (entity in test_ents):
          if entity in nomask.keys():
            nomask[entity] += 1
            nomask_true[entity] +=1
          else: nomask_true[entity] = 0; nomask[entity]=0
        elif entity in nomask_true.keys(): nomask_true[entity]+=1
        else: nomask_true[entity] = 0

  mask_true = {}
  mask = {}

  for review in masked_test_data:
    test_ents_true = [review[0][start:end] for (start, end, label) in review[1]['entities']]
    doc = trained_model(review[0])
    test_ents = [ent.text for ent in doc.ents]

    for entity in masked_entities:
      if entity in test_ents_true: 
        if (entity in test_ents):
          if entity in mask.keys():
            mask[entity] += 1
            mask_true[entity] +=1
          else: mask_true[entity] = 0; mask[entity]=0
        elif entity in mask_true.keys(): mask_true[entity]+=1
        else: mask_true[entity] = 0

  ratios_without_mask = {}
  for key in nomask.keys():
    if nomask_true[key] !=0:
      ratios_without_mask[key] = nomask[key]/nomask_true[key]
  ratios = {}
  for key in mask.keys():
    if mask_true[key] !=0:
      ratios[key] = mask[key]/mask_true[key]

  difference = {}
  for keys in ratios_without_mask:
    difference[keys] =  ratios[keys] - ratios_without_mask[keys]
  return difference, ratios, ratios_without_mask

def main(trained_model_dir='/content/drive/My Drive/ermodel', dataset_path="/content/drive/My Drive/spacy_train_clean.csv",verbose=True):
  df = pd.read_csv(dataset_path)
  df['entities_clean']=[ast.literal_eval(i) for i in df['entities_clean']]
  train_df, test_df = train_test_split(df, test_size = .2)
  trained_model = spacy.load(trained_model_dir))
  LABEL = "PRODUCT"
  start = time.time()
  masked_TRAIN_DATA, masked_TEST_DATA, masked_entities, unique_brands, newunique_brands = masked_train_test(train_df, test_df)

  TRAIN_DATA = create_train_data(train_df)
  TEST_DATA = create_test_data(test_df)

  difference, ratios, ratios_without_mask = evaluate_novelty(trained_model, masked_TRAIN_DATA,masked_TEST_DATA,masked_entities, TRAIN_DATA,TEST_DATA)
  if verbose == True:
    print('DIFFERENCES')
    print(difference)
    print('RATIOS WITH MASK')
    print(ratios)
    print('RATIOS WITHOUT MASK')
    print(ratios_without_mask)
  d = {'difference': difference, 'ratios with mask':ratios,'ratios without mask': ratios_without_mask}
  df = pd.DataFrame(data=d)
  return df