{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example string to do mini tests on\n",
    "example = \"I just EASILY had the BEST lunch I've ever eaten!  It was THAT good!\\n\\nThe chicken tortilla soup was out of this world...light and delicate...fresh and HOT!!!\\nI had two fish tacos with no tortilla.  One was a regular fish taco and the other was a beer battered fish taco.l\\n\\nThe fire roasted salsa was EASILY the best salsa I have ever had, too!\\n\\nThis place is a serious gem!  I could go there every single day!\\n\\nThanks guys!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Benepar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import benepar\n",
    "benepar.download(\"benepar_en2\")\n",
    "parser = benepar.Parser(\"benepar_en2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_parser = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Entity Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "model_dir = \"./models\"\n",
    "nlp = spacy.load(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(nlp_model, text):\n",
    "    \"\"\"\n",
    "    Input nlp_model and text, retrieve a list of unique entities from the text.\n",
    "    \"\"\"\n",
    "    doc = nlp_model(text)\n",
    "    entities = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PRODUCT\":\n",
    "            entities.add(ent.text)\n",
    "    return list(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soup',\n",
       " 'lunch',\n",
       " 'tortilla',\n",
       " 'fish',\n",
       " 'taco',\n",
       " 'beer',\n",
       " 'salsa',\n",
       " 'gem',\n",
       " 'place',\n",
       " 'chicken']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_entity_list = get_entities(nlp, example)\n",
    "example_entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick fix\n",
    "#example_entity_list = ['chicken tortilla soup', 'fish tacos', 'tortilla', 'beer battered fish taco', 'fire roasted salasa', 'place']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I just EASILY had the BEST lunch I've ever eaten!  It was THAT good!\\n\\nThe chicken tortilla soup was out of this world...light and delicate...fresh and HOT!!!\\nI had two fish tacos with no tortilla.  One was a regular fish taco and the other was a beer battered fish taco.l\\n\\nThe fire roasted salsa was EASILY the best salsa I have ever had, too!\\n\\nThis place is a serious gem!  I could go there every single day!\\n\\nThanks guys!!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "def dependency_tree(review, entities, parser, output = 'split_min'):\n",
    "    #possible output = split_min, split_all, tree_min, tree_all -> split only uses sentence splitter, while tree takes into account tree structure\n",
    "    doc = parser(review)\n",
    "    \n",
    "    if output == 'split_min' or output == 'split_all':\n",
    "        clauses = list(doc.sents)\n",
    "    #length of every clause\n",
    "    \n",
    "    lenlist = [len(str(x)) for x in clauses]\n",
    "        \n",
    "    #sort\n",
    "    clauses = [str(x) for _, x in sorted(zip(lenlist,clauses),reverse=False)]\n",
    "    \n",
    "    \n",
    "    entity_with_clause = []\n",
    "    \n",
    "    if output == 'split_min':\n",
    "        for entity in entities:\n",
    "            for clause in clauses:\n",
    "                if entity in clause:\n",
    "                    entity_with_clause.append((entity,clause))\n",
    "                    break\n",
    "                    \n",
    "    if output == 'split_all':\n",
    "        for entity in entities:\n",
    "            clauselist = []\n",
    "            for clause in clauses:\n",
    "                if entity in clause:\n",
    "                    clauselist.append(clause)\n",
    "            entity_with_clause.append((entity,clauselist))\n",
    "    \n",
    "    return(entity_with_clause)\n",
    "\n",
    "\n",
    "def remove_nestings(lst): \n",
    "    output = []\n",
    "    \n",
    "    def remove_nestings_recursive(l):\n",
    "        for i in l: \n",
    "            if type(i) == list: \n",
    "                remove_nestings_recursive(i) \n",
    "            else: \n",
    "                output.append(i)\n",
    "    \n",
    "    remove_nestings_recursive(lst)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def continue_splitting(review,list_of_dividers):\n",
    "        \n",
    "    temp = list_of_dividers.copy()\n",
    "    l = [review]\n",
    "    while len(temp) > 0:\n",
    "        divider = temp.pop(0)\n",
    "        l_new = []\n",
    "        for i in l:\n",
    "            l_new += i.split(divider)\n",
    "        l = l_new\n",
    "    return l\n",
    "\n",
    "def join_clause(review, list_of_split_clauses, list_of_dividers):\n",
    "    output = []\n",
    "    loc_of_split_clauses = []\n",
    "    for clause in list_of_split_clauses:\n",
    "        loc_of_split_clauses.append(review.find(clause))\n",
    "    for divider in list_of_dividers:\n",
    "        print(divider)\n",
    "        loc_div = review.find(divider)\n",
    "        print(loc_div)\n",
    "        for i in range(len(loc_of_split_clauses)):\n",
    "            if loc_div > loc_of_split_clauses[i]:\n",
    "                print(loc_div,loc_of_split_clauses[i])\n",
    "                \n",
    "def join_partitions(long_review,entity_with_review):\n",
    "    loclist = []\n",
    "    for (_, clause) in entity_with_review:\n",
    "        loclist.append((long_review.find(clause),long_review.find(clause)+len(clause)))\n",
    "    starts = {i for (i,j) in loclist}\n",
    "    ends = {j for (i,j) in loclist}\n",
    "    starts.add(len(long_review))\n",
    "    newends = {}\n",
    "    for i in ends:\n",
    "        newends[i] = min([x for x in starts if x >= i])\n",
    "    for i in newends:\n",
    "        pass\n",
    "    new_entity_with_review = []\n",
    "    for i in range(len(loclist)):\n",
    "        tup = loclist[i]\n",
    "        entity = entity_with_review[i][0]\n",
    "        st = tup[0]\n",
    "        en = newends[tup[1]]\n",
    "        new_entity_with_review.append((entity,long_review[st:en]))\n",
    "    return new_entity_with_review\n",
    "\n",
    "def split_long_string(review):\n",
    "    num = len(review)\n",
    "    split_list = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    while num != end:\n",
    "        if num - end < 1000:\n",
    "            end = num\n",
    "            split_list.append(review[start:end])\n",
    "        else:\n",
    "            end = review[start:(start+1000)].rfind('.')\n",
    "            if end == -1:\n",
    "                end = review[start:(start+1000)].rfind(' ') #if no '.', space will do\n",
    "                if end == -1:\n",
    "                    end = min(start + 1000,num)\n",
    "            split_list.append(review[start:end])\n",
    "            start = end\n",
    "    print(start,end)\n",
    "    return(split_list)\n",
    "\n",
    "def split_review_naive(review,entities):\n",
    "    clauses = re.split('[.?!]',review)\n",
    "    lenlist = [len(x) for x in clauses]\n",
    "    clauses = [x for _, x in sorted(zip(lenlist,clauses),reverse=False)]\n",
    "    entity_with_clause = []\n",
    "    for entity in entities:\n",
    "        for clause in clauses:\n",
    "            if entity in clause:\n",
    "                entity_with_clause.append((entity,clause))\n",
    "                break\n",
    "    return(join_partitions(review,entity_with_clause))\n",
    "\n",
    "def min_tree(review, entitiess, parser, output = 'minimum'):\n",
    "    \n",
    "    #review is string, entities is list of strings, parser is parser object\n",
    "    # TODO: How well are each review punctuatd and so forth EDA\n",
    "    if output == 'partition':\n",
    "        full_review = ''\n",
    "        \n",
    "    treelist = []\n",
    "    lenlist = []\n",
    "    temp = review.split('\\n')\n",
    "    \n",
    "    if output == 'no_parse':\n",
    "        return(split_review_naive(review,entities))\n",
    "    \n",
    "    if len(review) > 1000:\n",
    "        split_reviews = split_long_string(review)\n",
    "    else:\n",
    "        split_reviews = [i for i in temp if len(i) > 1 and len(i) <= 1000 ]\n",
    "    \n",
    "    for rev in split_reviews:\n",
    "        if rev and rev.strip():\n",
    "            u = parser.parse(rev) # tree \n",
    "\n",
    "            if type(u) == str:\n",
    "                u = nltk.Tree.fromstring(u)\n",
    "\n",
    "            for s in u.subtrees(): # subtrees \n",
    "                if s.label() == 'S': # if sentence\n",
    "                    treelist += [s]\n",
    "                    lenlist += [len(s.leaves())] # how long clause\n",
    "                        \n",
    "            if output == 'partition':\n",
    "                full_review += ' '.join(u.leaves())\n",
    "\n",
    "    treelist = [x for _, x in sorted(zip(lenlist,treelist),reverse=False)] # sort by lenlisit\n",
    "    clauses = [' '.join(tree.leaves()) for tree in treelist]\n",
    "    if not clauses:\n",
    "        clauses.append(review)\n",
    "    entity_with_clause = []\n",
    "    \n",
    "    if output == 'all':\n",
    "        for entity in entities:\n",
    "            clauselist = []\n",
    "            for clause in clauses:\n",
    "                if entity in clause:\n",
    "                    clauselist.append(clause)\n",
    "            entity_with_clause.append((entity,clauselist))\n",
    "    \n",
    "    #TODO: create rules and test them\n",
    "    elif output == 'minimum':\n",
    "        for entity in entities:\n",
    "            for clause in clauses:\n",
    "                if entity in clause:\n",
    "                    entity_with_clause.append((entity,clause))\n",
    "                    break\n",
    "                    \n",
    "    elif output == 'partition':\n",
    "        #first find minimal clause\n",
    "        for entity in entities:\n",
    "            for clause in clauses:\n",
    "                if entity in clause:\n",
    "                    entity_with_clause.append((entity,clause))\n",
    "                    break\n",
    "        #get location of minimal clause in review\n",
    "        \n",
    "        \n",
    "        entity_with_clause = join_partitions(full_review,entity_with_clause)\n",
    "    \n",
    "    return entity_with_clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Perform Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STANFORD NLP\n",
    "import numpy as np\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "#VADER\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "def vader_sentiment(entity_with_clause):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    entity_with_sentiment = []\n",
    "    for entity, clause in entity_with_clause:\n",
    "        sentiment = analyzer.polarity_scores(clause)['compound']\n",
    "        entity_with_sentiment.append((entity,sentiment))\n",
    "    return(entity_with_sentiment)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Werk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def werk(review, entities, parser = [], sentiment_package = 'vader', parse_package = 'benepar', rule = 'rule_1'):\n",
    "    \n",
    "    #print(\"\\nLoading Parser\")\n",
    "    \n",
    "    #first is the parser\n",
    "\n",
    "\n",
    "    #distinguish between constituency and dependency rules\n",
    "    \n",
    "    #dependency\n",
    "    dependency_rules = ['rule_7','rule_8']\n",
    "    if rule in dependency_rules:\n",
    "        if parse_package != 'spacy' and parse_package != 'stanford':\n",
    "            raise Exception('Dependency parsing requires spacy or stanford as parser')\n",
    "        if not parser and parse_package == 'spacy':\n",
    "            parser = spacy.load(\"en_core_web_sm\")\n",
    "        elif not parser and parse_package == 'stanford':\n",
    "            #parser = StanfordNLP('http://localhost')\n",
    "            raise Exception('not supported')\n",
    "        else:\n",
    "            pass\n",
    "    #constituency    \n",
    "    if not parser and parse_package == 'benepar':\n",
    "        parser = benepar.Parser(\"benepar_en2\")\n",
    "    elif not parser and parse_package == 'stanford':\n",
    "        #parser = StanfordNLP('http://localhost')\n",
    "        raise Exception('not supported')\n",
    "    elif parser:\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception('incorrect parse package')\n",
    "    \n",
    "\n",
    "        \n",
    "    #second is the rule\n",
    "    \n",
    "    #print(\"\\nLoading Rule\")\n",
    "    \n",
    "    if rule == 'rule_1':\n",
    "        \n",
    "        #print(\"Rule =\",rule)\n",
    "        \n",
    "        entity_with_review = min_tree(review, entities, parser, output = 'minimum')\n",
    "        \n",
    "        #print(\"\\nTree Generated\")\n",
    "        \n",
    "        entity_with_sentiment = sentiment_analysis(entity_with_review, sentiment_package)\n",
    "        \n",
    "        #print(\"\\nSentiment Generated\")\n",
    "\n",
    "        \n",
    "    elif rule == 'rule_2':\n",
    "        \n",
    "        #print(\"Rule =\",rule)\n",
    "        \n",
    "        entity_with_review = min_tree(review, entities, parser, output = 'all')\n",
    "        \n",
    "        #print(\"\\nTree Generated\")\n",
    "        entity_with_sentiment = []\n",
    "        sentiment = 0\n",
    "        for ent, revlist in entity_with_review:\n",
    "            for clause in revlist:\n",
    "                sentiment = sentiment_analysis_indiv(clause,sentiment_package)\n",
    "                if sentiment_package == 'benepar' and abs(sentiment) != 0:\n",
    "                    break\n",
    "                elif sentiment_package == 'stanford' and abs(sentiment) > 0.5:\n",
    "                    break\n",
    "                    #if sentiment is not neutral, stop. If sentiment is neutral, keep going up tree.\n",
    "            entity_with_sentiment.append((ent,sentiment))\n",
    "        #print(\"\\nSentiment Generated\")        \n",
    "        \n",
    "    elif rule == 'rule_3':\n",
    "        \n",
    "        #print(\"Rule =\",rule)\n",
    "        \n",
    "        entity_with_review = min_tree(review, entities, parser, output = 'all')\n",
    "        \n",
    "        #print(\"\\nTree Generated\")\n",
    "        \n",
    "        entity_with_sentiment = []\n",
    "        for ent, revlist in entity_with_review:\n",
    "            sentiment_list = []\n",
    "            for clause in revlist:\n",
    "                sentiment = sentiment_analysis_indiv(clause,sentiment_package)\n",
    "                sentiment_list.append(sentiment)\n",
    "            if not sentiment_list:\n",
    "                sentiment_list.append(0)\n",
    "            entity_with_sentiment.append((ent,np.mean(sentiment_list)))\n",
    "            \n",
    "        #print(\"\\nSentiment Generated\") \n",
    "        \n",
    "    elif rule == 'rule_4':\n",
    "        \n",
    "        #print(\"Rule =\",rule)\n",
    "        \n",
    "        entity_with_review = min_tree(review, entities, parser, output = 'partition')\n",
    "        #print(\"\\nTree Generated\")\n",
    "        \n",
    "        entity_with_sentiment = sentiment_analysis(entity_with_review, sentiment_package)\n",
    "        \n",
    "        #print(\"\\nSentiment Generated\")\n",
    "        \n",
    "    elif rule == 'rule_5':\n",
    "        \n",
    "        #print(\"Rule =\",rule)\n",
    "        \n",
    "        entity_with_review = min_tree(review, entities, parser, output = 'minimum')\n",
    "        entity_with_review_p = min_tree(review, entities, parser, output = 'partition')\n",
    "        #print(\"\\nTree Generated\")\n",
    "        \n",
    "        entity_with_sentiment = sentiment_analysis(entity_with_review, sentiment_package)\n",
    "        for i in range(len(entity_with_sentiment)):\n",
    "            sent = entity_with_sentiment[i][1]\n",
    "            if sentiment_package == 'vader' and sent != 0:\n",
    "                entity_with_sentiment[i] = (entity_with_sentiment[i][0],sentiment_analysis_indiv(entity_with_review_p[i][1],sentiment_package))\n",
    "            elif sentiment_package == 'stanford' and abs(sent) > 0.5:\n",
    "                entity_with_sentiment[i] = (entity_with_sentiment[i][0],sentiment_analysis_indiv(entity_with_review_p[i][1],sentiment_package))\n",
    "    \n",
    "    elif rule == 'rule_6':\n",
    "        \n",
    "        entity_with_review = min_tree(review, entities, parser, output = 'no_parse')\n",
    "        entity_with_sentiment = sentiment_analysis(entity_with_review, sentiment_package)\n",
    "        \n",
    "        #print(\"\\nSentiment Generated\")\n",
    "    \n",
    "    elif rule == 'rule_7':\n",
    "        entity_with_review = dependency_tree(review, entities, parser, output = 'split_min')\n",
    "        entity_with_sentiment = sentiment_analysis(entity_with_review, sentiment_package)\n",
    "        \n",
    "    elif rule == 'rule_8':\n",
    "        entity_with_review = dependency_tree(review, entities, parser, output = 'split_all')\n",
    "        \n",
    "        #print(\"\\nTree Generated\")\n",
    "        entity_with_sentiment = []\n",
    "        sentiment = 0\n",
    "        for ent, revlist in entity_with_review:\n",
    "            for clause in revlist:\n",
    "                sentiment = sentiment_analysis_indiv(clause,sentiment_package)\n",
    "                if sentiment_package == 'benepar' and abs(sentiment) != 0:\n",
    "                    break\n",
    "                elif sentiment_package == 'stanford' and abs(sentiment) > 0.5:\n",
    "                    break\n",
    "                    #if sentiment is not neutral, stop. If sentiment is neutral, keep going up tree.\n",
    "            entity_with_sentiment.append((ent,sentiment))\n",
    "    \n",
    "    else:\n",
    "        raise Exception('incorrect rule')\n",
    "        \n",
    "    \n",
    "    \n",
    "    return(entity_with_sentiment)\n",
    "    \n",
    "def sentiment_analysis(entity_with_review, sentiment_package = 'stanford'):\n",
    "    #takes in list of tuples\n",
    "    if sentiment_package == 'stanford':\n",
    "        return stanford_sentiment(entity_with_review)\n",
    "    elif sentiment_package == 'vader':\n",
    "        return vader_sentiment(entity_with_review)\n",
    "    else:\n",
    "        raise Exception('incorrect sentiment package')\n",
    "\n",
    "def sentiment_analysis_indiv(clause,sentiment_package = 'stanford'):\n",
    "    #takes in a single review\n",
    "    if sentiment_package == 'stanford':\n",
    "        nlp = stanford_sentiment_start()\n",
    "        result = nlp.annotate(clause,\n",
    "                   properties={\n",
    "                       'annotators': 'sentiment',\n",
    "                       'outputFormat': 'json'\n",
    "                   })\n",
    "        print(result['sentences'][0]['sentimentDistribution'])\n",
    "        return np.dot(result['sentences'][0]['sentimentDistribution'], [-2, -1, 0, 1, 2])\n",
    "    elif sentiment_package == 'vader':\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        return analyzer.polarity_scores(clause)['compound']\n",
    "    else:\n",
    "        raise Exception('incorrect sentiment package')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform End-to-End Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus = pd.read_csv(\"bus_same_stars_and_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_ids_similar_stars = bus.business_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RULE:  rule_8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544340ac0bd645b6ab11496ab6709231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on restaurant  HhVmDybpU7L50Kb5A0jXTg ...\n",
      "Number of Reviews left after subset length:  1600\n",
      "Extracting entities from each review...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9993c02b804d49bb8b84965203f8e9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1600), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering entities to have enough reviews...\n",
      "Calculating Yelp Star Rankings... \n",
      "Calculating Prediction Rankings...\n",
      "Performing sentiment analysis for each review... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e015fdae6b449d8b8a007520bc5132a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1600), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rankings result: \n",
      "        entity  average_stars  predicted_score\n",
      "0    appetizer       3.367589         0.080410\n",
      "1       burger       3.547432         0.286985\n",
      "2        fries       3.464529         0.293950\n",
      "3        bacon       3.631478         0.191861\n",
      "4        place       3.325500         0.271643\n",
      "5      service       3.419381         0.340006\n",
      "6       prices       3.549383         0.262945\n",
      "7         food       3.323452         0.332308\n",
      "8        table       2.919732         0.180771\n",
      "9         menu       3.347826         0.268475\n",
      "10       staff       3.508065         0.409546\n",
      "11      cheese       3.564815         0.264615\n",
      "12     special       3.163934         0.014320\n",
      "13         bit       3.387805         0.206358\n",
      "14        meat       2.943231         0.114870\n",
      "15        beer       3.432292         0.175356\n",
      "16        meal       3.326829         0.382112\n",
      "17       sauce       3.374723         0.321172\n",
      "18       lunch       3.620408         0.279918\n",
      "19   breakfast       3.484962         0.207472\n",
      "20      dinner       3.458537         0.244318\n",
      "21  atmosphere       4.126582         0.581086\n",
      "22       taste       2.909639         0.183333\n",
      "23        side       3.357259         0.199184\n",
      "24        pork       3.531532         0.249217\n",
      "25    sandwich       3.502232         0.243558\n",
      "26       price       3.142105         0.188267\n",
      "27       plate       3.040179         0.270134\n",
      "28      turkey       3.692771         0.225827\n",
      "29       salad       3.074627         0.239035\n",
      "30     chicken       3.301818         0.161100\n",
      "31     brisket       3.423077         0.143424\n",
      "32       chili       3.414286         0.187129\n",
      "33       sweet       3.477833         0.619884\n",
      "34     portion       3.786517         0.427120\n",
      "35       steak       3.196581         0.214748\n",
      "36     truffle       3.642857         0.200979\n",
      "Spearman Correlation Score:  0.40564248458985297\n",
      "Running on restaurant  FvVSy2r7_zDEhZWqLgjXNQ ...\n",
      "Number of Reviews left after subset length:  546\n",
      "Extracting entities from each review...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5878ba973cd4ec899cad4d9bb09f580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=546), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering entities to have enough reviews...\n",
      "Calculating Yelp Star Rankings... \n",
      "Calculating Prediction Rankings...\n",
      "Performing sentiment analysis for each review... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052261fc38c24eb7b059f232a94dea35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=546), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rankings result: \n",
      "       entity  average_stars  predicted_score\n",
      "0       lunch       4.163522         0.419718\n",
      "1       place       3.953668         0.388630\n",
      "2        food       3.737527         0.383086\n",
      "3       staff       3.639640         0.467068\n",
      "4   breakfast       4.187919         0.382454\n",
      "5      cheese       4.138710         0.368391\n",
      "6    sandwich       3.850365         0.348121\n",
      "7       bagel       4.070822         0.391907\n",
      "8        soup       4.144385         0.436603\n",
      "9     service       3.675900         0.340225\n",
      "10      cream       4.150000         0.388466\n",
      "Spearman Correlation Score:  0.10909090909090911\n",
      "Running on restaurant  A5Rkh7UymKm0_Rxm9K2PJw ...\n",
      "Number of Reviews left after subset length:  621\n",
      "Extracting entities from each review...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce2aa3650774979a92d7d97823425be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=621), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering entities to have enough reviews...\n",
      "Calculating Yelp Star Rankings... \n",
      "Calculating Prediction Rankings...\n",
      "Performing sentiment analysis for each review... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3913a014acff4150a06d70c40f4a699d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=621), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rankings result: \n",
      "        entity  average_stars  predicted_score\n",
      "0         food       3.861080         0.445124\n",
      "1      service       3.900840         0.463939\n",
      "2         beer       4.031949         0.486160\n",
      "3        place       3.920000         0.429337\n",
      "4         menu       4.080925         0.354035\n",
      "5        fries       3.879781         0.441991\n",
      "6        salad       3.892405         0.404670\n",
      "7      chicken       3.968051         0.269712\n",
      "8        lunch       3.845528         0.379182\n",
      "9        staff       3.906667         0.506309\n",
      "10  atmosphere       4.266667         0.596723\n",
      "11      cheese       3.991111         0.338532\n",
      "12      prices       4.009615         0.437044\n",
      "13      burger       3.905263         0.349420\n",
      "Spearman Correlation Score:  0.07692307692307693\n",
      "Running on restaurant  GIfZNMP0oIJCje_Xp0Bgrw ...\n",
      "Number of Reviews left after subset length:  377\n",
      "Extracting entities from each review...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a219d56255854229bd0ec7ea02bbe96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=377), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering entities to have enough reviews...\n",
      "Calculating Yelp Star Rankings... \n",
      "Calculating Prediction Rankings...\n",
      "Performing sentiment analysis for each review... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e34c3f883944dea91eb8ba48af5f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=377), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rankings result: \n",
      "       entity  average_stars  predicted_score\n",
      "0        food       3.597920         0.397494\n",
      "1     service       3.476190         0.372382\n",
      "2       staff       3.848684         0.492400\n",
      "3      burger       3.572650         0.210464\n",
      "4       place       3.607407         0.386404\n",
      "5  atmosphere       3.955224         0.508931\n",
      "6     chicken       3.903475         0.287398\n",
      "7        menu       3.811966         0.371234\n",
      "8        beer       3.742138         0.407603\n",
      "Spearman Correlation Score:  0.41666666666666663\n",
      "Running on restaurant  BjH8Xepc10i6OhCDQdX6og ...\n",
      "Number of Reviews left after subset length:  416\n",
      "Extracting entities from each review...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f99d529ca24bb6aae01ba5fcb9b248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=416), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering entities to have enough reviews...\n",
      "Calculating Yelp Star Rankings... \n",
      "Calculating Prediction Rankings...\n",
      "Performing sentiment analysis for each review... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f8cf03f62e4acea20a14b62384a544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=416), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rankings result: \n",
      "     entity  average_stars  predicted_score\n",
      "0      menu       3.544681         0.244628\n",
      "1     lunch       3.510638         0.093135\n",
      "2     place       3.454861         0.343848\n",
      "3     staff       3.740741         0.432620\n",
      "4   service       3.516791         0.345679\n",
      "5       pie       3.744681         0.222693\n",
      "6    banana       3.961749         0.290760\n",
      "7     cream       3.942308         0.262909\n",
      "8      food       3.514423         0.323843\n",
      "9      meal       3.627376         0.312777\n",
      "10   dinner       3.800000         0.277518\n",
      "11     fish       3.589577         0.282334\n",
      "Spearman Correlation Score:  -0.17482517482517487\n",
      "Running on restaurant  C8D_GU9cDDjbOJfCaGXxDQ ...\n",
      "Number of Reviews left after subset length:  694\n",
      "Extracting entities from each review...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdcd6c2b3f54e4687ca2108aecacf53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=694), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering entities to have enough reviews...\n",
      "Calculating Yelp Star Rankings... \n",
      "Calculating Prediction Rankings...\n",
      "Performing sentiment analysis for each review... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6d7c87f99f47f59e74804bc2aca80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=694), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rankings result: \n",
      "        entity  average_stars  predicted_score\n",
      "0        staff       3.891775         0.471018\n",
      "1      service       3.750000         0.480244\n",
      "2        place       3.834109         0.453307\n",
      "3       cheese       3.926020         0.269763\n",
      "4   atmosphere       4.047794         0.633762\n",
      "5         food       3.756155         0.460629\n",
      "6        table       3.401338         0.223416\n",
      "7       fondue       3.971098         0.330590\n",
      "8      pretzel       4.012704         0.326843\n",
      "9        salad       3.874539         0.351182\n",
      "10      dinner       3.776316         0.466698\n",
      "11       lunch       3.859259         0.329484\n",
      "12        menu       3.786096         0.366163\n",
      "13   appetizer       3.854460         0.276083\n",
      "14     chicken       3.948127         0.368622\n",
      "Spearman Correlation Score:  -0.024999999999999998\n",
      "Running on restaurant  FLMxWQO-ckCQmGZhU9OQgw ...\n",
      "Number of Reviews left after subset length:  515\n",
      "Extracting entities from each review...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc8fa75db314311ac1b32be0ac94139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=515), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering entities to have enough reviews...\n",
      "Calculating Yelp Star Rankings... \n",
      "Calculating Prediction Rankings...\n",
      "Performing sentiment analysis for each review... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0666bea4e2ad4c9f80715668429fab65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=515), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rankings result: \n",
      "     entity  average_stars  predicted_score\n",
      "0      crab       3.538642         0.207909\n",
      "1    buffet       3.478376         0.299863\n",
      "2   service       3.477509         0.422248\n",
      "3      food       3.407970         0.281594\n",
      "4   dessert       3.527964         0.310029\n",
      "5     lunch       3.488095         0.212718\n",
      "6    dinner       3.404959         0.173764\n",
      "7     place       3.361751         0.268002\n",
      "8   seafood       3.588745         0.177867\n",
      "9     price       3.358056         0.263172\n",
      "10      rib       3.325301         0.262558\n",
      "Spearman Correlation Score:  -0.10000000000000002\n",
      "Running on restaurant  -6tvduBzjLI1ISfs3F_qTg ...\n",
      "Number of Reviews left after subset length:  642\n",
      "Extracting entities from each review...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85376cf769e4c33a62b39ad00e9eef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=642), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering entities to have enough reviews...\n",
      "Calculating Yelp Star Rankings... \n",
      "Calculating Prediction Rankings...\n",
      "Performing sentiment analysis for each review... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9221149e20014e02a8ec809d2873a4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=642), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rankings result: \n",
      "        entity  average_stars  predicted_score\n",
      "0         fish       3.931655         0.306292\n",
      "1         food       3.675159         0.483400\n",
      "2        place       3.890130         0.465361\n",
      "3      service       3.786982         0.471555\n",
      "4   atmosphere       4.022124         0.600220\n",
      "5        salsa       3.646497         0.483316\n",
      "6         taco       3.926329         0.361581\n",
      "7        staff       4.005714         0.549307\n",
      "8        lunch       3.926316         0.446686\n",
      "9      burrito       3.813793         0.390769\n",
      "10     chicken       4.006494         0.243139\n",
      "11        corn       4.023810         0.437533\n",
      "Spearman Correlation Score:  -0.2027972027972028\n",
      "Running on restaurant  5LNZ67Yw9RD6nf4_UhXOjw ...\n",
      "Number of Reviews left after subset length:  730\n",
      "Extracting entities from each review...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94c09644935477da43b0a0a91a86977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=730), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering entities to have enough reviews...\n",
      "Calculating Yelp Star Rankings... \n",
      "Calculating Prediction Rankings...\n",
      "Performing sentiment analysis for each review... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0fa677305a48bea935adf2b6f5f316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=730), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rankings result: \n",
      "    entity  average_stars  predicted_score\n",
      "0    staff       3.724490         0.505158\n",
      "1  service       3.539623         0.409277\n",
      "2    place       3.856981         0.385927\n",
      "3     food       3.795337         0.509031\n",
      "Spearman Correlation Score:  -0.19999999999999998\n",
      "Running on restaurant  G-5kEa6E6PD5fkBRuA7k9Q ...\n",
      "Number of Reviews left after subset length:  509\n",
      "Extracting entities from each review...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee995aa09114927addb88ca52191939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=509), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering entities to have enough reviews...\n",
      "Calculating Yelp Star Rankings... \n",
      "Calculating Prediction Rankings...\n",
      "Performing sentiment analysis for each review... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e46092ccfbb4a0a874a9a2f25c4301a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=509), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rankings result: \n",
      "        entity  average_stars  predicted_score\n",
      "0         menu       3.518847         0.225776\n",
      "1        bread       3.578431         0.302147\n",
      "2         food       3.410433         0.324898\n",
      "3        place       3.363431         0.304671\n",
      "4      service       3.484646         0.375333\n",
      "5       dinner       3.521053         0.350042\n",
      "6        staff       3.403433         0.493951\n",
      "7         meal       3.472296         0.330833\n",
      "8        table       3.306818         0.355778\n",
      "9        lemon       3.889197         0.196133\n",
      "10  atmosphere       3.876033         0.498126\n",
      "11         bit       3.657692         0.169490\n",
      "12       pasta       3.481383         0.233992\n",
      "13       lunch       3.419811         0.226925\n",
      "Spearman Correlation Score:  -0.3142857142857143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "rules = ['rule_1', 'rule_2', 'rule_3', 'rule_4', 'rule_5', 'rule_6']\n",
    "\n",
    "rule = 'rule_8'\n",
    "\n",
    "correlation_scores = []\n",
    "\n",
    "print(\"RULE: \", rule)\n",
    "for bus_id in tqdm(business_ids_similar_stars):\n",
    "    print(\"Running on restaurant \", bus_id, \"...\")\n",
    "    subset = bus[bus.business_id == bus_id]\n",
    "    \n",
    "    # only get reviews with enough amount of text\n",
    "    reviews_subset = [review for review in subset.text if len(review) < 400]\n",
    "\n",
    "    print(\"Number of Reviews left after subset length: \", len(reviews_subset))\n",
    "    \n",
    "    # get set of entities for this particular restaurant,\n",
    "    # and count how many reviews each entity have\n",
    "    entities_with_count = defaultdict(int) \n",
    "    review_entities = [] # extract entities for each review\n",
    "    print(\"Extracting entities from each review...\")\n",
    "    for review in tqdm(reviews_subset):\n",
    "        entities = get_entities(nlp, review)\n",
    "\n",
    "        # add this review as a count to an entity\n",
    "        for ent in entities:\n",
    "            entities_with_count[ent.lower()] += 1\n",
    "\n",
    "        review_entities.append(entities)\n",
    "        \n",
    "    # only grab entities that have enough reviews\n",
    "    print(\"Filtering entities to have enough reviews...\")\n",
    "    entities_with_enough_reviews = []\n",
    "    threshold = 30\n",
    "    for key, value in entities_with_count.items():\n",
    "        if value >= threshold:\n",
    "            entities_with_enough_reviews.append(key)\n",
    "            \n",
    "    # TRUE RANKINGS CALCULATION\n",
    "    # for each entity, average ratings\n",
    "    true_rankings = defaultdict(list)\n",
    "\n",
    "    print(\"Calculating Yelp Star Rankings... \")\n",
    "    for entity in entities_with_enough_reviews:\n",
    "        true_rankings['entity'] += [entity]\n",
    "        entity_reviews = subset[subset.text.str.contains(entity, case=False)]\n",
    "        true_rankings['average_stars'] += [np.mean(entity_reviews.stars)]\n",
    "\n",
    "    true_rankings = pd.DataFrame(true_rankings)\n",
    "    \n",
    "    # PREDICTION RANKING CALCULATION\n",
    "    print(\"Calculating Prediction Rankings...\")\n",
    "    # Filter entities of each review to be from the entities_with_enough_review set\n",
    "    entity_filter = set(entities_with_enough_reviews)\n",
    "\n",
    "    filtered_entities = []\n",
    "\n",
    "    for entities in review_entities:\n",
    "        filtered = []\n",
    "        for ent in entities:\n",
    "            ent = ent.lower()\n",
    "            if ent in entity_filter:\n",
    "                filtered.append(ent)\n",
    "        filtered_entities.append(filtered)\n",
    "    \n",
    "#     # run validation for each rule\n",
    "#     for rule in rules:\n",
    "\n",
    "    # perform sentiment analysis for each review with filtered entities above\n",
    "    predicted_scores = defaultdict(list)\n",
    "\n",
    "    print(\"Performing sentiment analysis for each review... \")\n",
    "    for i, review in enumerate(tqdm(reviews_subset)):\n",
    "        entities = filtered_entities[i]\n",
    "\n",
    "    #     print(review)\n",
    "\n",
    "        scores = werk(review, \n",
    "                      entities, \n",
    "                      parser = spacy_parser, \n",
    "                      sentiment_package='vader',\n",
    "                      parse_package = 'spacy', \n",
    "                      rule=rule)\n",
    "\n",
    "        # save results \n",
    "        for entity, score in scores:\n",
    "            predicted_scores[entity] += [score]\n",
    "\n",
    "    # create rankings from scores\n",
    "    predicted_rankings = defaultdict(list)\n",
    "    for entity, scores in predicted_scores.items():\n",
    "        predicted_rankings['entity'] += [entity]\n",
    "        predicted_rankings['predicted_score'] += [np.mean(scores)]\n",
    "\n",
    "    predicted_rankings = pd.DataFrame(predicted_rankings)\n",
    "\n",
    "    #### may not be necessary to do these castings\n",
    "    predicted_rankings['entity'] = predicted_rankings['entity'].astype(str)\n",
    "    true_rankings['entity'] = true_rankings['entity'].astype(str)\n",
    "    ####\n",
    "    \n",
    "    full_rankings = true_rankings.merge(predicted_rankings, how='left').fillna(0)\n",
    "\n",
    "    # spearman correlation metric\n",
    "    print(\"Rankings result: \")\n",
    "    print(full_rankings)\n",
    "    \n",
    "    corr, pvalue = spearmanr(full_rankings.average_stars, full_rankings.predicted_score)\n",
    "    print(\"Spearman Correlation Score: \", corr)\n",
    "    correlation_scores.append(corr)\n",
    "        #     print(werk(review, entities, parser = parser,sentiment_package='vader'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Correlation Score:  -0.000858495463758624\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Correlation Score: \", np.mean(correlation_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
