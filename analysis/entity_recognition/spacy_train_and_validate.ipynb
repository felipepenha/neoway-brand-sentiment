{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to run spacy training. \n",
    "\n",
    "Change data path to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kmd5lFuiie0K"
   },
   "outputs": [],
   "source": [
    "import ast \n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import time\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nwpcWq6Byeob"
   },
   "outputs": [],
   "source": [
    "def create_train_data(df):\n",
    "  train_data = []\n",
    "  newnlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "  for i in range(len(df)):\n",
    "    doc = newnlp(df['text'].iloc[i])\n",
    "    entity_list = df['entities_clean'].iloc[i]\n",
    "    for ent in doc.ents:\n",
    "      entity_list.append((ent.start_char, ent.end_char, ent.label_))\n",
    "    entity_dict = {\"entities\": entity_list}\n",
    "    train_data.append((df['text'].iloc[i], entity_dict))\n",
    "  return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hLpYSX44z3rp"
   },
   "outputs": [],
   "source": [
    "def create_test_data(df):\n",
    "  test_data = []\n",
    "  newnlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "  for i in range(len(df)):\n",
    "    doc = newnlp(df['text'].iloc[i])\n",
    "    entity_list = df['entities_clean'].iloc[i]\n",
    "    for ent in doc.ents:\n",
    "      entity_list.append((ent.start_char, ent.end_char, ent.label_))\n",
    "    entity_dict = {\"entities\": entity_list}\n",
    "    test_data.append((df['text'].iloc[i], entity_dict))\n",
    "  return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NkUagu4twR-Y",
    "outputId": "89a39238-3500-4722-de90-267084ee32b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_core_web_sm'\n",
      "Total training time: 169.23812222480774\n",
      "Entities in 'new korean restaurant in north york neighbourhood. they offers topokki (rice cake) in three kinds sauces, red (spicy), white (creamy), and brown (korean bbq sauce). they also have few appetizers, and bingsoo (shaved ice).\n",
      "\n",
      "we order brown topokki in brown sauce with bulgolgi, with added cheese (for additional $2). the taste is not too spectacular but decent, like other korean restaurants nearby. but i like the innovation of the restaurant, a good combination of traditional korean food, snacks, and bingsoo. it has something for everyone. the decoration is very modern and bright, lots of seats. service is very attentive, and they give you a feedback form to fill out after meal.\n",
      "\n",
      "i would recommend this place to others.'\n",
      "PRODUCT rice\n",
      "PRODUCT cake\n",
      "CARDINAL three\n",
      "PRODUCT white\n",
      "PRODUCT sauce\n",
      "PRODUCT ice\n",
      "PRODUCT sauce\n",
      "PRODUCT bulgolgi\n",
      "PRODUCT cheese\n",
      "MONEY additional $2\n",
      "PRODUCT taste\n",
      "PRODUCT food\n",
      "PRODUCT meal\n",
      "Entities in '12/31/15 & 1/1/16 i love shopping at dsw. \n",
      "\n",
      "i'm the type that does not expect too much customer service and prefers to be left alone.  i do expect the workers to know their job. \n",
      "\n",
      "i originally was going to give an average three star rating because; the store was clean and organized, clearance rack was neat w/good selection of shoes,  store had many associates on the floor.  no stars for customer service because no initial greeting,  no asking if i needed assistance. \n",
      "\n",
      "for this visit, in the clearance section i found an ugg boot but it was snug.  \n",
      "\n",
      "1st associate-i asked if he could check inventory for bigger size... he told me to go check the clearance rack for bigger size.   i did but no luck, so i also began checking the floor. \n",
      "\n",
      "2nd associate in boot area,  i asked if there were any on the floor.  she gave me 3 reasons,  why this boot would only be in the clearance section.   one reason,  was this was a return that was purchased at another store and they do not carry this style.  i thought this girl knows her inventory.   anyway,  i found the boot on the sales floor and got the size i needed.   sigh...\n",
      "\n",
      "3rd associate-i'm paying for the boot and asked the associate if the 3x point promotion expired.   she gave me a questioning look so i decided to look for the email on my phone.   takes me a minute to look and i see it had expired.  so as i tell her it expired she tells me, \"oh,  yeah that expired a couple of days ago.\"   sigh... \n",
      "\n",
      "so i purchase the boots.   i decided to wear the boots the next day and i'm setting off security sensors at other stores,  i finally figured out it was because the security sensor from the boots was not removed.  we drive back to dsw, and i go directly to the cash register.   the cashier,  3rd associate,  recognizes me and the boots and immediately removes the sensor.   then she makez the comment,  how come the alarm didn't go off when i entered the store with the boots.  \n",
      "\n",
      "anyway, i minused 2 stars because the 3 associates wasted my time.'\n",
      "CARDINAL 12/31/15\n",
      "CARDINAL three\n",
      "PRODUCT rack\n",
      "ORDINAL 1st\n",
      "PRODUCT rack\n",
      "ORDINAL 2nd\n",
      "CARDINAL 3\n",
      "CARDINAL one\n",
      "ORDINAL 3rd\n",
      "TIME a minute\n",
      "DATE a couple of days ago\n",
      "DATE the next day\n",
      "ORDINAL 3rd\n",
      "CARDINAL 2\n",
      "CARDINAL 3\n",
      "Entities in 'loved the food! the biggest show stoppers for me were the turkey dumplings and the wagyu beef.  omg... so delicious, so flavourful, everything about both dishes were absolute perfection. they were like flavour explosions! the kentucky fried squid was very good as well. it had just the right amount of sauce on it, not drenched and not dry.... just right to give the right flavour and squid cooked perfectly!  we had the beef short rib which was tasty, but the dumplings and wagyu were so exceptional that i can't stop thinking about them! we also got the asparagus but it was too spicy for me (but i am a total spiciness wimp...i always get zero spiciness if i have a choice so i'm not a good judge of spicy stuff.) the key lime pie was beautiful and tasted great too, i loved the meringue pieces, yummy and beautiful! i'm really shocked at the bad reviews, the staff were super friendly and attentive and the food was amazing! maybe it was just growing pains of just opening? i definitely want to try their sunday family style next time!'\n",
      "PRODUCT food\n",
      "PRODUCT turkey\n",
      "PRODUCT dumplings\n",
      "PRODUCT wagyu\n",
      "PRODUCT beef\n",
      "PRODUCT delicious\n",
      "PRODUCT squid\n",
      "PRODUCT sauce\n",
      "PRODUCT squid\n",
      "PRODUCT beef\n",
      "PRODUCT rib\n",
      "PRODUCT dumplings\n",
      "PRODUCT asparagus\n",
      "CARDINAL zero\n",
      "PRODUCT pie\n",
      "PRODUCT food\n",
      "DATE sunday\n",
      "Entities in 'been here twice. one of the tastiest americanos i've ever had. so rich! not sure what they're doing right. free wifi and a fair bit of workspace. they just need more comfortable seats.'\n",
      "CARDINAL one\n",
      "PRODUCT bit\n",
      "Entities in 'eggslut is always an la fave for me. \n",
      "bringing anything, from anywhere, to vegas, i feel, just loses their shine. \n",
      "it might not be vegas, but these places that are known to taste amazing somewhere else, just doesn't cut it when it's here. \n",
      "but hey, still great if you've never tried the original!'\n",
      "PRODUCT taste\n",
      "PRODUCT cut\n",
      "Entities in 'i've tried this place several times and each time i'm very disappointed. overpriced food that is under cooked and overdressed. when you order a dish with salmon and quinoa and vegetables they bring you your salmon in a cup which i think is ridiculous. staff is careless doesn't pay attention. this place needs a lot of help!!! if i were you i wouldn't even bother going.'\n",
      "PRODUCT food\n",
      "PRODUCT dish\n",
      "PRODUCT salmon\n",
      "PRODUCT salmon\n",
      "PRODUCT cup\n",
      "Entities in 'this place is great! warm friendly staff and the owner becky is always around to make you feel welcome. excellent food and live music too! highly recommend ! enjoy!  burgers are huge, best italian wedding soup in town!'\n",
      "PRODUCT food\n",
      "PRODUCT soup\n",
      "Entities in 'the food was absolutely amazing!! atmosphere was spot on! or waiter was boarder line and there was a super loud birthday party (which obviously is not the restaurants fault) my dinner took a long  time to come out, which they made up for with a free glass of wine! this is a great great stop! a must...'\n",
      "PRODUCT food\n",
      "PRODUCT dinner\n",
      "PRODUCT wine\n",
      "PRODUCT must\n",
      "Entities in 'first off, i've got to say that the location and decor of this place is awesome. great job by whoever is responsible for that. this is a great place for a date, or for brunch with friends on the weekends. the food is excellent and the service is outstanding. i would go more often, but it is a little on the pricy side... but well worth it.'\n",
      "ORDINAL first\n",
      "PRODUCT date\n",
      "PRODUCT food\n",
      "PRODUCT side\n",
      "Entities in 'i don't mind the earl's chain, the menu items always entice me and they are reasonably priced for what you get. i decided to have lunch here while working downtown as local was completely full. surprisingly, we were greeted by two friendly hostesses and our server that afternoon was very friendly and attentive.\n",
      "\n",
      "we ordered my favourite appie, the dynamite rolls, and they were delicious! no complaints there. i also went with the tortilla soup and the chicken caesar salad, both were quite good and very generous portions. based on the poor reviews, i was expecting a lackluster meal, but i was actually impressed and we were in and out within the hour. i may just give this earl's another try next time i am dining dt.'\n",
      "PRODUCT menu\n",
      "PRODUCT lunch\n",
      "CARDINAL two\n",
      "TIME afternoon\n",
      "PRODUCT delicious\n",
      "PRODUCT tortilla\n",
      "PRODUCT soup\n",
      "PRODUCT chicken\n",
      "PRODUCT salad\n",
      "PRODUCT meal\n",
      "TIME the hour\n",
      "Saved model to /content/drive/My Drive/ermodel\n",
      "Loading from /content/drive/My Drive/ermodel\n",
      "PRODUCT menu\n",
      "PRODUCT lunch\n",
      "CARDINAL two\n",
      "TIME afternoon\n",
      "PRODUCT delicious\n",
      "PRODUCT tortilla\n",
      "PRODUCT soup\n",
      "PRODUCT chicken\n",
      "PRODUCT salad\n",
      "PRODUCT meal\n",
      "TIME the hour\n"
     ]
    }
   ],
   "source": [
    "# new entity label\n",
    "def train(train_data=TRAIN_DATA, test_data=TEST_DATA, model='en_core_web_sm', new_model_name=\"product\", output_dir='/content/drive/My Drive/ermodel', n_iter=1,verbose=True):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    random.seed(0)\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    ner.add_label(LABEL)  # add new entity label to entity recognizer\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    move_names = list(ner.move_names)\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        # batch the examples using spaCy's minibatch\n",
    "        start = time.time()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)            \n",
    "            #print(\"Training Recall:\",nlp.evaluate(random.sample(TRAIN_DATA,200)).ents_r)\n",
    "            #print(\"Test Recall:\",nlp.evaluate(TEST_DATA).ents_p) #COMMENT: isn't this precision?\n",
    "            #COMMENT: so test data here is evaluating test_data which has the format \n",
    "            # of e.g. (\"Uber blew through $1 million a week\", {\"entities\": [(0, 4, \"ORG\")]}) right\n",
    "            #print(\"Training Losses\", losses)\n",
    "        end = time.time()\n",
    "    print(\"Total training time:\",end-start)\n",
    "\n",
    "    # test the trained model (small sample test)\n",
    "    if verbose==True:\n",
    "      for i in range(10):\n",
    "        print('Sample predictions from 10 reviews in test set:')\n",
    "        test_text = test_data[i][0]\n",
    "        doc = nlp(test_text)\n",
    "        print(\"Entities in '%s'\" % test_text)\n",
    "        for ent in doc.ents:\n",
    "            print(ent.label_, ent.text)\n",
    "\n",
    "    # COMMENT: Abstract to another function\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta[\"name\"] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # COMMENT: Abstract to another function \n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        # Check the classes have loaded back consistently\n",
    "        assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NdUs3pwlv6q"
   },
   "outputs": [],
   "source": [
    "def main(datapath='/content/drive/My Drive/spacy_train_clean.csv',output_dir='/content/drive/My Drive/ermodel', verbose=True)\n",
    "  df = pd.read_csv(datapath)\n",
    "  df['entities_clean']=[ast.literal_eval(i) for i in df['entities_clean']]\n",
    "  train_df, test_df = train_test_split(df, test_size = .2)\n",
    "  LABEL = \"PRODUCT\"\n",
    "  TRAIN_DATA = create_train_data(train_df)\n",
    "  TEST_DATA = create_test_data(test_df)\n",
    "  model = train(TRAIN_DATA,TEST_DATA,output_dir=output_dir,verbose=verbose)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Validate Spacy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"spacy_validate.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1KTr0oUxy27VOldpmjxfs-zf5nGwIwmaf\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "import ast \n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import time\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def create_train_data(df):\n",
    "  train_data = []\n",
    "  newnlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "  for i in range(len(df)):\n",
    "    doc = newnlp(df['text'].iloc[i])\n",
    "    entity_list = df['entities_clean'].iloc[i]\n",
    "    for ent in doc.ents:\n",
    "      entity_list.append((ent.start_char, ent.end_char, ent.label_))\n",
    "    entity_dict = {\"entities\": entity_list}\n",
    "    train_data.append((df['text'].iloc[i], entity_dict))\n",
    "  return train_data\n",
    "\n",
    "def create_test_data(df):\n",
    "  test_data = []\n",
    "  newnlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "  for i in range(len(df)):\n",
    "    doc = newnlp(df['text'].iloc[i])\n",
    "    entity_list = df['entities_clean'].iloc[i]\n",
    "    for ent in doc.ents:\n",
    "      entity_list.append((ent.start_char, ent.end_char, ent.label_))\n",
    "    entity_dict = {\"entities\": entity_list}\n",
    "    test_data.append((df['text'].iloc[i], entity_dict))\n",
    "  return test_data\n",
    "\n",
    "def create_masked_train_data(df, masked_entities):\n",
    "  train_data = []\n",
    "  newnlp = spacy.load(\"en_core_web_sm\")\n",
    "  \n",
    "  for i in range(len(df)):\n",
    "    doc = newnlp(df['text'].iloc[i])\n",
    "    entity_list = df['entities_clean'].iloc[i]\n",
    "    for ent in doc.ents:\n",
    "      if ent.text not in masked_entities:\n",
    "        entity_list.append((ent.start_char, ent.end_char, ent.label_))\n",
    "    entity_dict = {\"entities\": entity_list}\n",
    "    train_data.append((df['text'].iloc[i], entity_dict))\n",
    "  return train_data\n",
    "\n",
    "def masked_train_test(train, test):\n",
    "  brand_list = []\n",
    "  for (index,entity_loc) in enumerate(train['entities_clean']):\n",
    "    text = train['text'].iloc[index]\n",
    "    for pair in entity_loc:\n",
    "      brand_list.append(text[pair[0]:pair[1]])\n",
    "      \n",
    "  import numpy as np\n",
    "  unique_brands = np.unique(brand_list)\n",
    "\n",
    "  newbrand_list = []\n",
    "  for (index, entity_loc) in enumerate(test['entities_clean']):\n",
    "    text = test['text'].iloc[index]\n",
    "    for pair in entity_loc:\n",
    "      newbrand_list.append(text[pair[0]:pair[1]])\n",
    "      \n",
    "  import numpy as np\n",
    "  newunique_brands = np.unique(newbrand_list)\n",
    "\n",
    "  in_common = list(set(unique_brands) & set(newunique_brands))\n",
    "  print(\"Total in common:\",len(in_common))\n",
    "\n",
    "  masked_entities, unmasked_entities = train_test_split(in_common, test_size = .5)\n",
    "  print(\"Total masked:\", len(masked_entities))\n",
    "\n",
    "  # new entity label\n",
    "  TRAIN_DATA = create_masked_train_data(train, masked_entities)\n",
    "  TEST_DATA = create_test_data(test)\n",
    "  return TRAIN_DATA, TEST_DATA, masked_entities, unique_brands, newunique_brands\n",
    "\n",
    "def evaluate_novelty(trained_model, masked_train_data, masked_test_data, masked_entities, unmasked_train_data, unmasked_test_data):\n",
    "  nomask_true = {}\n",
    "  nomask = {}\n",
    "\n",
    "  for review in unmasked_test_data:\n",
    "    test_ents_true = [review[0][start:end] for (start, end, label) in review[1]['entities']]\n",
    "    doc = trained_model(review[0])\n",
    "    test_ents = [ent.text for ent in doc.ents]\n",
    "\n",
    "    for entity in masked_entities:\n",
    "      if entity in test_ents_true: \n",
    "        if (entity in test_ents):\n",
    "          if entity in nomask.keys():\n",
    "            nomask[entity] += 1\n",
    "            nomask_true[entity] +=1\n",
    "          else: nomask_true[entity] = 0; nomask[entity]=0\n",
    "        elif entity in nomask_true.keys(): nomask_true[entity]+=1\n",
    "        else: nomask_true[entity] = 0\n",
    "\n",
    "  mask_true = {}\n",
    "  mask = {}\n",
    "\n",
    "  for review in masked_test_data:\n",
    "    test_ents_true = [review[0][start:end] for (start, end, label) in review[1]['entities']]\n",
    "    doc = trained_model(review[0])\n",
    "    test_ents = [ent.text for ent in doc.ents]\n",
    "\n",
    "    for entity in masked_entities:\n",
    "      if entity in test_ents_true: \n",
    "        if (entity in test_ents):\n",
    "          if entity in mask.keys():\n",
    "            mask[entity] += 1\n",
    "            mask_true[entity] +=1\n",
    "          else: mask_true[entity] = 0; mask[entity]=0\n",
    "        elif entity in mask_true.keys(): mask_true[entity]+=1\n",
    "        else: mask_true[entity] = 0\n",
    "\n",
    "  ratios_without_mask = {}\n",
    "  for key in nomask.keys():\n",
    "    if nomask_true[key] !=0:\n",
    "      ratios_without_mask[key] = nomask[key]/nomask_true[key]\n",
    "  ratios = {}\n",
    "  for key in mask.keys():\n",
    "    if mask_true[key] !=0:\n",
    "      ratios[key] = mask[key]/mask_true[key]\n",
    "\n",
    "  difference = {}\n",
    "  for keys in ratios_without_mask:\n",
    "    difference[keys] =  ratios[keys] - ratios_without_mask[keys]\n",
    "  return difference, ratios, ratios_without_mask\n",
    "\n",
    "def evaluate_spacy(trained_model_dir='./workspace/models/er_model', dataset_path=\"./workspace/data/test.csv\", verbose=True):\n",
    "  df = pd.read_csv(dataset_path)\n",
    "  df['entities_clean']=[ast.literal_eval(i) for i in df['entities']]\n",
    "  train_df, test_df = train_test_split(df, test_size = .2)\n",
    "  trained_model = spacy.load(trained_model_dir)\n",
    "  LABEL = \"PRODUCT\"\n",
    "  masked_TRAIN_DATA, masked_TEST_DATA, masked_entities, unique_brands, newunique_brands = masked_train_test(train_df, test_df)\n",
    "\n",
    "  TRAIN_DATA = create_train_data(train_df)\n",
    "  TEST_DATA = create_test_data(test_df)\n",
    "\n",
    "  difference, ratios, ratios_without_mask = evaluate_novelty(trained_model, masked_TRAIN_DATA,masked_TEST_DATA,masked_entities, TRAIN_DATA,TEST_DATA)\n",
    "  if verbose == True:\n",
    "    print('DIFFERENCES')\n",
    "    print(difference)\n",
    "    print('RATIOS WITH MASK')\n",
    "    print(ratios)\n",
    "    print('RATIOS WITHOUT MASK')\n",
    "    print(ratios_without_mask)\n",
    "  d = {'difference': difference, 'ratios with mask':ratios,'ratios without mask': ratios_without_mask}\n",
    "  df = pd.DataFrame(data=d)\n",
    "  return df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "spacy_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
