{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example string to do mini tests on\n",
    "example = \"I just EASILY had the BEST lunch I've ever eaten!  It was THAT good!\\n\\nThe chicken tortilla soup was out of this world...light and delicate...fresh and HOT!!!\\nI had two fish tacos with no tortilla.  One was a regular fish taco and the other was a beer battered fish taco.l\\n\\nThe fire roasted salsa was EASILY the best salsa I have ever had, too!\\n\\nThis place is a serious gem!  I could go there every single day!\\n\\nThanks guys!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Benepar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "[nltk_data] Downloading package benepar_en2 to\n",
      "[nltk_data]     /Users/TimGimi/nltk_data...\n",
      "[nltk_data]   Package benepar_en2 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/benepar/base_parser.py:197: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/benepar/base_parser.py:202: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import benepar\n",
    "benepar.download(\"benepar_en2\")\n",
    "parser = benepar.Parser(\"benepar_en2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import StanfordNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to first instantiate stanfordNLP in java!\n",
    "\n",
    "java command: \n",
    "\n",
    "``\n",
    "java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators \"tokenize,ssplit,pos,lemma,parse,sentiment\" -sentiment.threads 8 -port 9000 -timeout 30000``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse: (ROOT\n",
      "  (S\n",
      "    (NP (DT A) (NN blog))\n",
      "    (VP (NN post)\n",
      "      (S\n",
      "        (VP (VBG using)\n",
      "          (NP (NNP Stanford) (NNP CoreNLP) (NN Server)))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import logging\n",
    "import json\n",
    "\n",
    "class StanfordNLP:\n",
    "    def __init__(self, host='http://localhost', port=9000):\n",
    "        self.nlp = StanfordCoreNLP(host, port=port,\n",
    "                                   timeout=30000)  # , quiet=False, logging_level=logging.DEBUG)\n",
    "        self.props = {\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,ner,parse,depparse,dcoref,relation',\n",
    "            'pipelineLanguage': 'en',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "\n",
    "    def word_tokenize(self, sentence):\n",
    "        return self.nlp.word_tokenize(sentence)\n",
    "\n",
    "    def pos(self, sentence):\n",
    "        return self.nlp.pos_tag(sentence)\n",
    "\n",
    "    def ner(self, sentence):\n",
    "        return self.nlp.ner(sentence)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        return self.nlp.parse(sentence)\n",
    "\n",
    "    def dependency_parse(self, sentence):\n",
    "        return self.nlp.dependency_parse(sentence)\n",
    "\n",
    "    def annotate(self, sentence):\n",
    "        return json.loads(self.nlp.annotate(sentence, properties=self.props))\n",
    "\n",
    "    @staticmethod\n",
    "    def tokens_to_dict(_tokens):\n",
    "        tokens = defaultdict(dict)\n",
    "        for token in _tokens:\n",
    "            tokens[int(token['index'])] = {\n",
    "                'word': token['word'],\n",
    "                'lemma': token['lemma'],\n",
    "                'pos': token['pos'],\n",
    "                'ner': token['ner']\n",
    "            }\n",
    "        return tokens\n",
    "\n",
    "    \n",
    "\n",
    "sNLP = StanfordNLP()\n",
    "text = 'A blog post using Stanford CoreNLP Server. Visit www.khalidalnajjar.com for more details.'\n",
    "print(\"Parse:\", sNLP.parse(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Entity Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "model_dir = \"./models\"\n",
    "nlp = spacy.load(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(nlp_model, text):\n",
    "    \"\"\"\n",
    "    Input nlp_model and text, retrieve a list of unique entities from the text.\n",
    "    \"\"\"\n",
    "    doc = nlp_model(text)\n",
    "    entities = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PRODUCT\":\n",
    "            entities.add(ent.text)\n",
    "    return list(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soup',\n",
       " 'chicken',\n",
       " 'fish',\n",
       " 'taco',\n",
       " 'place',\n",
       " 'lunch',\n",
       " 'gem',\n",
       " 'beer',\n",
       " 'tortilla',\n",
       " 'salsa']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_entity_list = get_entities(nlp, example)\n",
    "example_entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick fix\n",
    "#example_entity_list = ['chicken tortilla soup', 'fish tacos', 'tortilla', 'beer battered fish taco', 'fire roasted salasa', 'place']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I just EASILY had the BEST lunch I've ever eaten!  It was THAT good!\\n\\nThe chicken tortilla soup was out of this world...light and delicate...fresh and HOT!!!\\nI had two fish tacos with no tortilla.  One was a regular fish taco and the other was a beer battered fish taco.l\\n\\nThe fire roasted salsa was EASILY the best salsa I have ever had, too!\\n\\nThis place is a serious gem!  I could go there every single day!\\n\\nThanks guys!!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "def remove_nestings(lst): \n",
    "    output = []\n",
    "    \n",
    "    def remove_nestings_recursive(l):\n",
    "        for i in l: \n",
    "            if type(i) == list: \n",
    "                remove_nestings_recursive(i) \n",
    "            else: \n",
    "                output.append(i)\n",
    "    \n",
    "    remove_nestings_recursive(lst)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def continue_splitting(review,list_of_dividers):\n",
    "        \n",
    "    temp = list_of_dividers.copy()\n",
    "    l = [review]\n",
    "    while len(temp) > 0:\n",
    "        divider = temp.pop(0)\n",
    "        l_new = []\n",
    "        for i in l:\n",
    "            l_new += i.split(divider)\n",
    "        l = l_new\n",
    "    return l\n",
    "\n",
    "def join_clause(review, list_of_split_clauses, list_of_dividers):\n",
    "    output = []\n",
    "    loc_of_split_clauses = []\n",
    "    for clause in list_of_split_clauses:\n",
    "        loc_of_split_clauses.append(review.find(clause))\n",
    "    for divider in list_of_dividers:\n",
    "        print(divider)\n",
    "        loc_div = review.find(divider)\n",
    "        print(loc_div)\n",
    "        for i in range(len(loc_of_split_clauses)):\n",
    "            if loc_div > loc_of_split_clauses[i]:\n",
    "                print(loc_div,loc_of_split_clauses[i])\n",
    "                \n",
    "def join_partitions(long_review,entity_with_review):\n",
    "    loclist = []\n",
    "    for (_, clause) in entity_with_review:\n",
    "        loclist.append((long_review.find(clause),long_review.find(clause)+len(clause)))\n",
    "    starts = {i for (i,j) in loclist}\n",
    "    ends = {j for (i,j) in loclist}\n",
    "    starts.add(len(long_review))\n",
    "    newends = {}\n",
    "    for i in ends:\n",
    "        newends[i] = min([x for x in starts if x >= i])\n",
    "    for i in newends:\n",
    "        pass\n",
    "    new_entity_with_review = []\n",
    "    for i in range(len(loclist)):\n",
    "        tup = loclist[i]\n",
    "        entity = entity_with_review[i][0]\n",
    "        st = tup[0]\n",
    "        en = newends[tup[1]]\n",
    "        new_entity_with_review.append((entity,long_review[st:en]))\n",
    "    return new_entity_with_review\n",
    "\n",
    "def split_long_string(review):\n",
    "    num = len(review)\n",
    "    split_list = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    while num != end:\n",
    "        if num - end < 1000:\n",
    "            end = num\n",
    "            split_list.append(review[start:end])\n",
    "        else:\n",
    "            end = review[start:(start+1000)].rfind('.')\n",
    "            if end == -1:\n",
    "                end = review[start:(start+1000)].rfind(' ') #if no '.', space will do\n",
    "                if end == -1:\n",
    "                    end = min(start + 1000,num)\n",
    "            split_list.append(review[start:end])\n",
    "            start = end\n",
    "    print(start,end)\n",
    "    return(split_list)\n",
    "\n",
    "def split_review_naive(review,entities):\n",
    "    clauses = re.split('[.?!]',review)\n",
    "    lenlist = [len(x) for x in clauses]\n",
    "    clauses = [x for _, x in sorted(zip(lenlist,clauses),reverse=False)]\n",
    "    entity_with_clause = []\n",
    "    for entity in entities:\n",
    "        for clause in clauses:\n",
    "            if entity in clause:\n",
    "                entity_with_clause.append((entity,clause))\n",
    "                break\n",
    "    return(join_partitions(review,entity_with_clause))\n",
    "\n",
    "def min_tree(review, entitiess, parser, output = 'minimum'):\n",
    "    \n",
    "    #review is string, entities is list of strings, parser is parser object\n",
    "    # TODO: How well are each review punctuatd and so forth EDA\n",
    "    if output == 'partition':\n",
    "        full_review = ''\n",
    "        \n",
    "    treelist = []\n",
    "    lenlist = []\n",
    "    temp = review.split('\\n')\n",
    "    \n",
    "    if output == 'no_parse':\n",
    "        return(split_review_naive(review,entities))\n",
    "    \n",
    "    if len(review) > 1000:\n",
    "        split_reviews = split_long_string(review)\n",
    "    else:\n",
    "        split_reviews = [i for i in temp if len(i) > 1 and len(i) <= 1000 ]\n",
    "    \n",
    "    for rev in split_reviews:\n",
    "        if rev and rev.strip():\n",
    "            u = parser.parse(rev) # tree \n",
    "\n",
    "            if type(u) == str:\n",
    "                u = nltk.Tree.fromstring(u)\n",
    "\n",
    "            for s in u.subtrees(): # subtrees \n",
    "                if s.label() == 'S': # if sentence\n",
    "                    treelist += [s]\n",
    "                    lenlist += [len(s.leaves())] # how long clause\n",
    "                        \n",
    "            if output == 'partition':\n",
    "                full_review += ' '.join(u.leaves())\n",
    "\n",
    "    treelist = [x for _, x in sorted(zip(lenlist,treelist),reverse=False)] # sort by lenlisit\n",
    "    clauses = [' '.join(tree.leaves()) for tree in treelist]\n",
    "    if not clauses:\n",
    "        clauses.append(review)\n",
    "    entity_with_clause = []\n",
    "    \n",
    "    if output == 'all':\n",
    "        for entity in entities:\n",
    "            clauselist = []\n",
    "            for clause in clauses:\n",
    "                if entity in clause:\n",
    "                    clauselist.append(clause)\n",
    "            entity_with_clause.append((entity,clauselist))\n",
    "    \n",
    "    #TODO: create rules and test them\n",
    "    elif output == 'minimum':\n",
    "        for entity in entities:\n",
    "            for clause in clauses:\n",
    "                if entity in clause:\n",
    "                    entity_with_clause.append((entity,clause))\n",
    "                    break\n",
    "                    \n",
    "    elif output == 'partition':\n",
    "        #first find minimal clause\n",
    "        for entity in entities:\n",
    "            for clause in clauses:\n",
    "                if entity in clause:\n",
    "                    entity_with_clause.append((entity,clause))\n",
    "                    break\n",
    "        #get location of minimal clause in review\n",
    "        \n",
    "        \n",
    "        entity_with_clause = join_partitions(full_review,entity_with_clause)\n",
    "    \n",
    "    return entity_with_clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Perform Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STANFORD NLP\n",
    "import numpy as np\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "def stanford_sentiment_start():\n",
    "    nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "    return nlp\n",
    "\n",
    "def stanford_sentiment(entity_with_clause):\n",
    "    nlp = stanford_sentiment_start()\n",
    "    entity_with_sentiment = []\n",
    "    for entity, clause in entity_with_clause:\n",
    "        result = nlp.annotate(clause,\n",
    "                   properties={\n",
    "                       'annotators': 'sentiment',\n",
    "                       'outputFormat': 'json'\n",
    "                   })\n",
    "        sentiment = np.dot(result['sentences'][0]['sentimentDistribution'], [-2, -1, 0, 1, 2])\n",
    "        entity_with_sentiment.append((entity, sentiment))\n",
    "    return entity_with_sentiment\n",
    "\n",
    "#VADER\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "def vader_sentiment(entity_with_clause):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    entity_with_sentiment = []\n",
    "    for entity, clause in entity_with_clause:\n",
    "        sentiment = analyzer.polarity_scores(clause)['compound']\n",
    "        entity_with_sentiment.append((entity,sentiment))\n",
    "    return(entity_with_sentiment)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Werk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def werk(review, entities, parser = [], sentiment_package = 'vader', parse_package = 'benepar', rule = 'rule_1'):\n",
    "    \n",
    "    #print(\"\\nLoading Parser\")\n",
    "    \n",
    "    #first is the parser\n",
    "    if not parser and parse_package == 'benepar':\n",
    "        parser = benepar.Parser(\"benepar_en2\")\n",
    "    elif not parser and parse_package == 'stanford':\n",
    "        #parser = StanfordNLP('http://localhost')\n",
    "        raise Exception('incorrect parse package')\n",
    "    elif parser:\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception('incorrect parse package')\n",
    "    \n",
    "    #print(\"Parser =\", parse_package)\n",
    "\n",
    "        \n",
    "    #second is the rule\n",
    "    \n",
    "    #print(\"\\nLoading Rule\")\n",
    "    \n",
    "    if rule == 'rule_1':\n",
    "        \n",
    "        #print(\"Rule =\",rule)\n",
    "        \n",
    "        entity_with_review = min_tree(review, entities, parser, output = 'minimum')\n",
    "        \n",
    "        #print(\"\\nTree Generated\")\n",
    "        \n",
    "        entity_with_sentiment = sentiment_analysis(entity_with_review, sentiment_package)\n",
    "        \n",
    "        #print(\"\\nSentiment Generated\")\n",
    "\n",
    "        \n",
    "    elif rule == 'rule_2':\n",
    "        \n",
    "        #print(\"Rule =\",rule)\n",
    "        \n",
    "        entity_with_review = min_tree(review, entities, parser, output = 'all')\n",
    "        \n",
    "        #print(\"\\nTree Generated\")\n",
    "        entity_with_sentiment = []\n",
    "        sentiment = 0\n",
    "        for ent, revlist in entity_with_review:\n",
    "            for clause in revlist:\n",
    "                sentiment = sentiment_analysis_indiv(clause,sentiment_package)\n",
    "                if sentiment_package == 'benepar' and abs(sentiment) != 0:\n",
    "                    break\n",
    "                elif sentiment_package == 'stanford' and abs(sentiment) > 0.5:\n",
    "                    break\n",
    "                    #if sentiment is not neutral, stop. If sentiment is neutral, keep going up tree.\n",
    "            entity_with_sentiment.append((ent,sentiment))\n",
    "        #print(\"\\nSentiment Generated\")        \n",
    "        \n",
    "    elif rule == 'rule_3':\n",
    "        \n",
    "        #print(\"Rule =\",rule)\n",
    "        \n",
    "        entity_with_review = min_tree(review, entities, parser, output = 'all')\n",
    "        \n",
    "        #print(\"\\nTree Generated\")\n",
    "        \n",
    "        entity_with_sentiment = []\n",
    "        for ent, revlist in entity_with_review:\n",
    "            sentiment_list = []\n",
    "            for clause in revlist:\n",
    "                sentiment = sentiment_analysis_indiv(clause,sentiment_package)\n",
    "                sentiment_list.append(sentiment)\n",
    "            entity_with_sentiment.append((ent,np.mean(sentiment_list)))\n",
    "            \n",
    "        #print(\"\\nSentiment Generated\") \n",
    "        \n",
    "    elif rule == 'rule_4':\n",
    "        \n",
    "        #print(\"Rule =\",rule)\n",
    "        \n",
    "        entity_with_review = min_tree(review, entities, parser, output = 'partition')\n",
    "        #print(\"\\nTree Generated\")\n",
    "        \n",
    "        entity_with_sentiment = sentiment_analysis(entity_with_review, sentiment_package)\n",
    "        \n",
    "        #print(\"\\nSentiment Generated\")\n",
    "        \n",
    "    elif rule == 'rule_5':\n",
    "        \n",
    "        #print(\"Rule =\",rule)\n",
    "        \n",
    "        entity_with_review = min_tree(review, entities, parser, output = 'minimum')\n",
    "        entity_with_review_p = min_tree(review, entities, parser, output = 'partition')\n",
    "        #print(\"\\nTree Generated\")\n",
    "        \n",
    "        entity_with_sentiment = sentiment_analysis(entity_with_review, sentiment_package)\n",
    "        for i in range(len(entity_with_sentiment)):\n",
    "            sent = entity_with_sentiment[i][1]\n",
    "            if sentiment_package == 'vader' and sent != 0:\n",
    "                entity_with_sentiment[i] = (entity_with_sentiment[i][0],sentiment_analysis_indiv(entity_with_review_p[i][1],sentiment_package))\n",
    "            elif sentiment_package == 'stanford' and abs(sent) > 0.5:\n",
    "                entity_with_sentiment[i] = (entity_with_sentiment[i][0],sentiment_analysis_indiv(entity_with_review_p[i][1],sentiment_package))\n",
    "    \n",
    "    elif rule == 'rule_6':\n",
    "        \n",
    "        entity_with_review = min_tree(review, entities, parser, output = 'no_parse')\n",
    "        entity_with_sentiment = sentiment_analysis(entity_with_review, sentiment_package)\n",
    "        \n",
    "        #print(\"\\nSentiment Generated\")\n",
    "    \n",
    "    else:\n",
    "        raise Exception('incorrect rule')\n",
    "    \n",
    "    return(entity_with_sentiment)\n",
    "    \n",
    "def sentiment_analysis(entity_with_review, sentiment_package = 'stanford'):\n",
    "    #takes in list of tuples\n",
    "    if sentiment_package == 'stanford':\n",
    "        return stanford_sentiment(entity_with_review)\n",
    "    elif sentiment_package == 'vader':\n",
    "        return vader_sentiment(entity_with_review)\n",
    "    else:\n",
    "        raise Exception('incorrect sentiment package')\n",
    "\n",
    "def sentiment_analysis_indiv(clause,sentiment_package = 'stanford'):\n",
    "    #takes in a single review\n",
    "    if sentiment_package == 'stanford':\n",
    "        nlp = stanford_sentiment_start()\n",
    "        result = nlp.annotate(clause,\n",
    "                   properties={\n",
    "                       'annotators': 'sentiment',\n",
    "                       'outputFormat': 'json'\n",
    "                   })\n",
    "        print(result['sentences'][0]['sentimentDistribution'])\n",
    "        return np.dot(result['sentences'][0]['sentimentDistribution'], [-2, -1, 0, 1, 2])\n",
    "    elif sentiment_package == 'vader':\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        return analyzer.polarity_scores(clause)['compound']\n",
    "    else:\n",
    "        raise Exception('incorrect sentiment package')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform End-to-End Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus = pd.read_csv(\"bus_same_stars_and_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_ids_similar_stars = bus.business_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RULE:  rule_3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a0c573d56e4cc391524287ac67111c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on restaurant  HhVmDybpU7L50Kb5A0jXTg ...\n",
      "Number of Reviews left after subset length:  1600\n",
      "Extracting entities from each review...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47ac3c3094a4ffe9d21628485bc87a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1600), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering entities to have enough reviews...\n",
      "Calculating Yelp Star Rankings... \n",
      "Calculating Prediction Rankings...\n",
      "Performing sentiment analysis for each review... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ce4c38eca54768a0f8f4493657fa62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1600), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-bb4253eb9647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m#     print(review)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwerk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment_package\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'vader'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-dd0281857323>\u001b[0m in \u001b[0;36mwerk\u001b[0;34m(review, entities, parser, sentiment_package, parse_package, rule)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m#print(\"Rule =\",rule)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mentity_with_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m#print(\"\\nTree Generated\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-fcbffd478d33>\u001b[0m in \u001b[0;36mmin_tree\u001b[0;34m(review, entitiess, parser, output)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrev\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_reviews\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrev\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/benepar/nltk_plugin.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \"\"\"\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/benepar/nltk_plugin.py\u001b[0m in \u001b[0;36mparse_sents\u001b[0;34m(self, sents)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mparse_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batched_parsed_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nltk_process_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_nltk_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mparse_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/benepar/base_parser.py\u001b[0m in \u001b[0;36m_batched_parsed_raw\u001b[0;34m(self, sentence_data_pairs)\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchart_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_charts_and_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mchart_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchart_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/benepar/base_parser.py\u001b[0m in \u001b[0;36m_make_charts_and_tags\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_provides_tags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0mcharts_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_charts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mcharts_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_charts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "rules = ['rule_1', 'rule_2', 'rule_3', 'rule_4', 'rule_5', 'rule_6']\n",
    "\n",
    "rule = 'rule_1'\n",
    "\n",
    "correlation_scores = []\n",
    "\n",
    "print(\"RULE: \", rule)\n",
    "for bus_id in tqdm(business_ids_similar_stars):\n",
    "    print(\"Running on restaurant \", bus_id, \"...\")\n",
    "    subset = bus[bus.business_id == bus_id]\n",
    "    \n",
    "    # only get reviews with enough amount of text\n",
    "    reviews_subset = [review for review in subset.text if len(review) < 400]\n",
    "\n",
    "    print(\"Number of Reviews left after subset length: \", len(reviews_subset))\n",
    "    \n",
    "    # get set of entities for this particular restaurant,\n",
    "    # and count how many reviews each entity have\n",
    "    entities_with_count = defaultdict(int) \n",
    "    review_entities = [] # extract entities for each review\n",
    "    print(\"Extracting entities from each review...\")\n",
    "    for review in tqdm(reviews_subset):\n",
    "        entities = get_entities(nlp, review)\n",
    "\n",
    "        # add this review as a count to an entity\n",
    "        for ent in entities:\n",
    "            entities_with_count[ent.lower()] += 1\n",
    "\n",
    "        review_entities.append(entities)\n",
    "        \n",
    "    # only grab entities that have enough reviews\n",
    "    print(\"Filtering entities to have enough reviews...\")\n",
    "    entities_with_enough_reviews = []\n",
    "    threshold = 30\n",
    "    for key, value in entities_with_count.items():\n",
    "        if value >= threshold:\n",
    "            entities_with_enough_reviews.append(key)\n",
    "            \n",
    "    # TRUE RANKINGS CALCULATION\n",
    "    # for each entity, average ratings\n",
    "    true_rankings = defaultdict(list)\n",
    "\n",
    "    print(\"Calculating Yelp Star Rankings... \")\n",
    "    for entity in entities_with_enough_reviews:\n",
    "        true_rankings['entity'] += [entity]\n",
    "        entity_reviews = subset[subset.text.str.contains(entity, case=False)]\n",
    "        true_rankings['average_stars'] += [np.mean(entity_reviews.stars)]\n",
    "\n",
    "    true_rankings = pd.DataFrame(true_rankings)\n",
    "    \n",
    "    # PREDICTION RANKING CALCULATION\n",
    "    print(\"Calculating Prediction Rankings...\")\n",
    "    # Filter entities of each review to be from the entities_with_enough_review set\n",
    "    entity_filter = set(entities_with_enough_reviews)\n",
    "\n",
    "    filtered_entities = []\n",
    "\n",
    "    for entities in review_entities:\n",
    "        filtered = []\n",
    "        for ent in entities:\n",
    "            ent = ent.lower()\n",
    "            if ent in entity_filter:\n",
    "                filtered.append(ent)\n",
    "        filtered_entities.append(filtered)\n",
    "    \n",
    "#     # run validation for each rule\n",
    "#     for rule in rules:\n",
    "\n",
    "    # perform sentiment analysis for each review with filtered entities above\n",
    "    predicted_scores = defaultdict(list)\n",
    "\n",
    "    print(\"Performing sentiment analysis for each review... \")\n",
    "    for i, review in enumerate(tqdm(reviews_subset)):\n",
    "        entities = filtered_entities[i]\n",
    "\n",
    "    #     print(review)\n",
    "\n",
    "        scores = werk(review, entities, parser = parser, sentiment_package='vader', rule=rule)\n",
    "\n",
    "        # save results \n",
    "        for entity, score in scores:\n",
    "            predicted_scores[entity] += [score]\n",
    "\n",
    "    # create rankings from scores\n",
    "    predicted_rankings = defaultdict(list)\n",
    "    for entity, scores in predicted_scores.items():\n",
    "        predicted_rankings['entity'] += [entity]\n",
    "        predicted_rankings['predicted_score'] += [np.mean(scores)]\n",
    "\n",
    "    predicted_rankings = pd.DataFrame(predicted_rankings)\n",
    "\n",
    "    #### may not be necessary to do these castings\n",
    "    predicted_rankings['entity'] = predicted_rankings['entity'].astype(str)\n",
    "    true_rankings['entity'] = true_rankings['entity'].astype(str)\n",
    "    ####\n",
    "    \n",
    "    full_rankings = true_rankings.merge(predicted_rankings, how='left').fillna(0)\n",
    "\n",
    "    # spearman correlation metric\n",
    "    print(\"Rankings result: \")\n",
    "    print(full_rankings)\n",
    "    \n",
    "    corr, pvalue = spearmanr(full_rankings.average_stars, full_rankings.predicted_score)\n",
    "    print(\"Spearman Correlation Score: \", corr)\n",
    "    correlation_scores.append(corr)\n",
    "        #     print(werk(review, entities, parser = parser,sentiment_package='vader'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Correlation Score: \", np.mean(correlation_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
